{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating $B = A * W$, where both $A$ and $W$ are non-transposed (NN).\n",
    "\n",
    "Refer the [Conv2d Cuda code](https://tvm.apache.org/docs/how_to/optimize_operators/opt_conv_cuda.html#sphx-glr-how-to-optimize-operators-opt-conv-cuda-py) to implement GEMM, i.e. GEMM is equivalent to Conv2d when $Ix=Iy=Kx=Ky=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math, time\n",
    "import sys\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import tvm\n",
    "import tvm.testing\n",
    "from tvm import te\n",
    "import numpy\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primfn(A_1: handle, W_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [8192, 8192], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [8192, 8192], []),\n",
      "             W: Buffer(W_2: Pointer(float32), float32, [8192, 8192], [])}\n",
      "  buffer_map = {A_1: A, W_1: W, B_1: B} {\n",
      "  allocate(A.shared: Pointer(shared float32), float32, [32768]), storage_scope = shared;\n",
      "  allocate(A.shared.local: Pointer(local float32), float32, [32768]), storage_scope = local;\n",
      "  allocate(W.shared.local: Pointer(local float32), float32, [32768]), storage_scope = local;\n",
      "  allocate(B.local: Pointer(local float32), float32, [16]), storage_scope = local {\n",
      "    for (ax0: int32, 0, 4) {\n",
      "      for (ax1: int32, 0, 8192) {\n",
      "        A.shared[((ax0*8192) + ax1)] = (float32*)A_2[(((((blockIdx.y: int32*524288) + (vy: int32*262144)) + (threadIdx.y: int32*32768)) + (ax0*8192)) + ax1)]\n",
      "      }\n",
      "    }\n",
      "    for (ax0_1: int32, 0, 4) {\n",
      "      for (ax1_1: int32, 0, 8192) {\n",
      "        A.shared.local[((ax0_1*8192) + ax1_1)] = (float32*)A.shared[((ax0_1*8192) + ax1_1)]\n",
      "      }\n",
      "    }\n",
      "    for (ax0_2: int32, 0, 8192) {\n",
      "      for (ax1_2: int32, 0, 4) {\n",
      "        A.shared[((ax0_2*4) + ax1_2)] = (float32*)W_2[(((((ax0_2*8192) + (blockIdx.x: int32*64)) + (vx: int32*32)) + (threadIdx.x: int32*4)) + ax1_2)]\n",
      "      }\n",
      "    }\n",
      "    for (ax0_3: int32, 0, 8192) {\n",
      "      for (ax1_3: int32, 0, 4) {\n",
      "        W.shared.local[((ax0_3*4) + ax1_3)] = (float32*)A.shared[((ax0_3*4) + ax1_3)]\n",
      "      }\n",
      "    }\n",
      "    for (i.c: int32, 0, 4) {\n",
      "      for (j.c: int32, 0, 4) {\n",
      "        B.local[((i.c*4) + j.c)] = 0f32\n",
      "        for (rk: int32, 0, 8192) {\n",
      "          B.local[((i.c*4) + j.c)] = ((float32*)B.local[((i.c*4) + j.c)] + ((float32*)A.shared.local[((i.c*8192) + rk)]*(float32*)W.shared.local[((rk*4) + j.c)]))\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    attr [IterVar(blockIdx.x, (nullptr), \"ThreadIndex\", \"blockIdx.x\")] \"thread_extent\" = 128;\n",
      "    attr [IterVar(blockIdx.y, (nullptr), \"ThreadIndex\", \"blockIdx.y\")] \"thread_extent\" = 128;\n",
      "    attr [IterVar(threadIdx.x, [0:8], \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 8;\n",
      "    attr [IterVar(threadIdx.y, [0:8], \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 8;\n",
      "    for (i.inner.inner.inner: int32, 0, 4) {\n",
      "      for (j.inner.inner.inner: int32, 0, 4) {\n",
      "        B_2[((((((blockIdx.y*524288) + (threadIdx.y*32768)) + (i.inner.inner.inner*8192)) + (blockIdx.x*64)) + (threadIdx.x*4)) + j.inner.inner.inner)] = (float32*)B.local[((i.inner.inner.inner*4) + j.inner.inner.inner)]\n",
      "        B_2[(((((((blockIdx.y*524288) + (threadIdx.y*32768)) + (i.inner.inner.inner*8192)) + (blockIdx.x*64)) + (threadIdx.x*4)) + j.inner.inner.inner) + 32)] = (float32*)B.local[((i.inner.inner.inner*4) + j.inner.inner.inner)]\n",
      "        B_2[(((((((blockIdx.y*524288) + (threadIdx.y*32768)) + (i.inner.inner.inner*8192)) + (blockIdx.x*64)) + (threadIdx.x*4)) + j.inner.inner.inner) + 262144)] = (float32*)B.local[((i.inner.inner.inner*4) + j.inner.inner.inner)]\n",
      "        B_2[(((((((blockIdx.y*524288) + (threadIdx.y*32768)) + (i.inner.inner.inner*8192)) + (blockIdx.x*64)) + (threadIdx.x*4)) + j.inner.inner.inner) + 262176)] = (float32*)B.local[((i.inner.inner.inner*4) + j.inner.inner.inner)]\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "primfn(A_1: handle, W_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {B: Buffer(B_2: Pointer(float32), float32, [8192, 8192], []),\n",
      "             A: Buffer(A_2: Pointer(float32), float32, [8192, 8192], []),\n",
      "             W: Buffer(W_2: Pointer(float32), float32, [8192, 8192], [])}\n",
      "  buffer_map = {A_1: A, W_1: W, B_1: B} {\n",
      "  attr [IterVar(blockIdx.x: int32, (nullptr), \"ThreadIndex\", \"blockIdx.x\")] \"thread_extent\" = 128;\n",
      "  allocate(B.local: Pointer(local float32), float32, [64]), storage_scope = local;\n",
      "  allocate(A.shared: Pointer(shared float32), float32, [512]), storage_scope = shared;\n",
      "  allocate(W.shared: Pointer(shared float32), float32, [512]), storage_scope = shared;\n",
      "  allocate(A.shared.local: Pointer(local float32), float32, [8]), storage_scope = local;\n",
      "  allocate(W.shared.local: Pointer(local float32), float32, [8]), storage_scope = local;\n",
      "  attr [IterVar(blockIdx.y: int32, (nullptr), \"ThreadIndex\", \"blockIdx.y\")] \"thread_extent\" = 128;\n",
      "  attr [IterVar(threadIdx.x: int32, [0:8], \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 8;\n",
      "  attr [IterVar(threadIdx.y: int32, [0:8], \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 8 {\n",
      "    for (i.c.init: int32, 0, 4) {\n",
      "      for (j.c.init: int32, 0, 4) {\n",
      "        B.local[((i.c.init*4) + j.c.init)] = 0f32\n",
      "        B.local[(((i.c.init*4) + j.c.init) + 32)] = 0f32\n",
      "        B.local[(((i.c.init*4) + j.c.init) + 16)] = 0f32\n",
      "        B.local[(((i.c.init*4) + j.c.init) + 48)] = 0f32\n",
      "      }\n",
      "    }\n",
      "    for (rk.outer: int32, 0, 1024) {\n",
      "      for (ax0.inner.outer: int32, 0, 2) {\n",
      "        A.shared[ramp((((threadIdx.x*64) + (ax0.inner.outer*32)) + threadIdx.y), 8, 4)] = (float32x4*)A_2[ramp((((((blockIdx.y*524288) + (threadIdx.x*65536)) + (ax0.inner.outer*32768)) + (rk.outer*8)) + threadIdx.y), 8192, 4)]\n",
      "      }\n",
      "      for (ax1.inner: int32, 0, 8) {\n",
      "        W.shared[(((threadIdx.x*64) + (threadIdx.y*8)) + ax1.inner)] = (float32*)W_2[(((((rk.outer*65536) + (threadIdx.x*8192)) + (blockIdx.x*64)) + (threadIdx.y*8)) + ax1.inner)]\n",
      "      }\n",
      "      for (rk.inner: int32, 0, 8) {\n",
      "        for (ax0: int32, 0, 4) {\n",
      "          A.shared.local[ax0] = (float32*)A.shared[(((threadIdx.y*32) + (ax0*8)) + rk.inner)]\n",
      "          A.shared.local[(ax0 + 4)] = (float32*)A.shared[((((threadIdx.y*32) + (ax0*8)) + rk.inner) + 256)]\n",
      "        }\n",
      "        for (ax1: int32, 0, 4) {\n",
      "          W.shared.local[ax1] = (float32*)W.shared[(((rk.inner*64) + (threadIdx.x*4)) + ax1)]\n",
      "          W.shared.local[(ax1 + 4)] = (float32*)W.shared[((((rk.inner*64) + (threadIdx.x*4)) + ax1) + 32)]\n",
      "        }\n",
      "        for (i.c: int32, 0, 4) {\n",
      "          for (j.c: int32, 0, 4) {\n",
      "            B.local[((i.c*4) + j.c)] = ((float32*)B.local[((i.c*4) + j.c)] + ((float32*)A.shared.local[i.c]*(float32*)W.shared.local[j.c]))\n",
      "            B.local[(((i.c*4) + j.c) + 32)] = ((float32*)B.local[(((i.c*4) + j.c) + 32)] + ((float32*)A.shared.local[i.c]*(float32*)W.shared.local[(j.c + 4)]))\n",
      "            B.local[(((i.c*4) + j.c) + 16)] = ((float32*)B.local[(((i.c*4) + j.c) + 16)] + ((float32*)A.shared.local[(i.c + 4)]*(float32*)W.shared.local[j.c]))\n",
      "            B.local[(((i.c*4) + j.c) + 48)] = ((float32*)B.local[(((i.c*4) + j.c) + 48)] + ((float32*)A.shared.local[(i.c + 4)]*(float32*)W.shared.local[(j.c + 4)]))\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (i.inner.inner.inner: int32, 0, 4) {\n",
      "      for (j.inner.inner.inner: int32, 0, 4) {\n",
      "        B_2[((((((blockIdx.y*524288) + (threadIdx.y*32768)) + (i.inner.inner.inner*8192)) + (blockIdx.x*64)) + (threadIdx.x*4)) + j.inner.inner.inner)] = (float32*)B.local[((i.inner.inner.inner*4) + j.inner.inner.inner)]\n",
      "        B_2[(((((((blockIdx.y*524288) + (threadIdx.y*32768)) + (i.inner.inner.inner*8192)) + (blockIdx.x*64)) + (threadIdx.x*4)) + j.inner.inner.inner) + 32)] = (float32*)B.local[(((i.inner.inner.inner*4) + j.inner.inner.inner) + 32)]\n",
      "        B_2[(((((((blockIdx.y*524288) + (threadIdx.y*32768)) + (i.inner.inner.inner*8192)) + (blockIdx.x*64)) + (threadIdx.x*4)) + j.inner.inner.inner) + 262144)] = (float32*)B.local[(((i.inner.inner.inner*4) + j.inner.inner.inner) + 16)]\n",
      "        B_2[(((((((blockIdx.y*524288) + (threadIdx.y*32768)) + (i.inner.inner.inner*8192)) + (blockIdx.x*64)) + (threadIdx.x*4)) + j.inner.inner.inner) + 262176)] = (float32*)B.local[(((i.inner.inner.inner*4) + j.inner.inner.inner) + 48)]\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "MatMul: 130.656734 ms, 8.45 TFLOPS\n"
     ]
    }
   ],
   "source": [
    "m, n, k = 8192, 8192, 8192\n",
    "A = te.placeholder((m, k), name=\"A\", dtype=\"float32\")\n",
    "W = te.placeholder((k, n), name=\"W\", dtype=\"float32\")\n",
    "rk = te.reduce_axis((0, k), name=\"rk\")\n",
    "B = te.compute(\n",
    "    (m, n), \n",
    "    lambda i, j: te.sum(A[i, rk].astype(\"float32\")*W[rk, j].astype(\"float32\"), axis=[rk]), \n",
    "    name=\"B\",\n",
    ")\n",
    "\n",
    "s = te.create_schedule(B.op)\n",
    "AA = s.cache_read(A, \"shared\", [B])\n",
    "WW = s.cache_read(W, \"shared\", [B])\n",
    "AL = s.cache_read(AA, \"local\", [B])\n",
    "WL = s.cache_read(WW, \"local\", [B])\n",
    "BL = s.cache_write(B, \"local\")\n",
    "\n",
    "tile = 8\n",
    "num_thread = 8\n",
    "block_factor = tile * num_thread\n",
    "step = 8\n",
    "vthread = 2\n",
    "block_x = te.thread_axis(\"blockIdx.x\")\n",
    "block_y = te.thread_axis(\"blockIdx.y\")\n",
    "thread_x = te.thread_axis((0, num_thread), \"threadIdx.x\")\n",
    "thread_y = te.thread_axis((0, num_thread), \"threadIdx.y\")\n",
    "thread_xz = te.thread_axis((0, vthread), \"vthread\", name=\"vx\")\n",
    "thread_yz = te.thread_axis((0, vthread), \"vthread\", name=\"vy\")\n",
    "\n",
    "mi, ni = s[B].op.axis\n",
    "by, mi = s[B].split(mi, factor=block_factor)\n",
    "bx, ni = s[B].split(ni, factor=block_factor)\n",
    "s[B].bind(bx, block_x)\n",
    "s[B].bind(by, block_y)\n",
    "\n",
    "tyz, mi = s[B].split(mi, nparts=vthread)\n",
    "txz, ni = s[B].split(ni, nparts=vthread)\n",
    "ty, mi = s[B].split(mi, nparts=num_thread)\n",
    "tx, ni = s[B].split(ni, nparts=num_thread)\n",
    "\n",
    "# s[B].reorder(by, bx, tyz, txz, ty, tx, ni, mi)\n",
    "s[B].reorder(bx, by, txz, tyz, tx, ty, mi, ni)\n",
    "\n",
    "s[B].bind(txz, thread_xz)\n",
    "s[B].bind(tyz, thread_yz)\n",
    "s[B].bind(tx, thread_x)\n",
    "s[B].bind(ty, thread_y)\n",
    "print(tvm.lower(s, [A, W, B], simple_mode=True))\n",
    "\n",
    "# s[BL].compute_at(s[B], tx)\n",
    "s[BL].compute_at(s[B], ty)\n",
    "mi, ni = s[BL].op.axis\n",
    "rk, = s[BL].op.reduce_axis\n",
    "\n",
    "rko, rki = s[BL].split(rk, factor=step)\n",
    "s[BL].reorder(rko, rki, mi, ni)\n",
    "s[AA].compute_at(s[BL], rko)\n",
    "s[WW].compute_at(s[BL], rko)\n",
    "s[AL].compute_at(s[BL], rki)\n",
    "s[WL].compute_at(s[BL], rki)\n",
    "\n",
    "# s[BL].reorder(rk, mi, ni)\n",
    "# s[AA].compute_at(s[BL], rk)\n",
    "# s[WW].compute_at(s[BL], rk)\n",
    "# s[AL].compute_at(s[BL], rk)\n",
    "# s[WL].compute_at(s[BL], rk)\n",
    "\n",
    "mi, ki = s[AA].op.axis\n",
    "tx, mi = s[AA].split(mi, nparts=num_thread)\n",
    "ty, ki = s[AA].split(ki, nparts=num_thread)\n",
    "_, mi = s[AA].split(mi, factor=4)\n",
    "s[AA].reorder(tx, ty, ki, mi)\n",
    "s[AA].bind(tx, thread_x)\n",
    "s[AA].bind(ty, thread_y)\n",
    "s[AA].vectorize(mi)\n",
    "\n",
    "ni, ki = s[WW].op.axis\n",
    "tx, ni = s[WW].split(ni, nparts=num_thread)\n",
    "ty, ki = s[WW].split(ki, nparts=num_thread)\n",
    "_, ni = s[WW].split(ni, factor=4)\n",
    "s[WW].reorder(tx, ty, ki, ni)\n",
    "s[WW].bind(tx, thread_x)\n",
    "s[WW].bind(ty, thread_y)\n",
    "s[WW].vectorize(ni)\n",
    "print(tvm.lower(s, [A, W, B], simple_mode=True))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "func = tvm.build(s, [A, W, B], \"cuda\")\n",
    "dev = tvm.cuda(0)\n",
    "a_np = np.random.uniform(size=(m,k)).astype(A.dtype)\n",
    "w_np = np.random.uniform(size=(k,n)).astype(W.dtype)\n",
    "b_np = np.zeros((m,n)).astype(B.dtype) \n",
    "\n",
    "a = tvm.nd.array(a_np, dev)\n",
    "w = tvm.nd.array(w_np, dev)\n",
    "b = tvm.nd.array(b_np, dev)\n",
    "func(a, w, b)\n",
    "evaluator = func.time_evaluator(func.entry_name, dev, number=1)\n",
    "latency = evaluator(a, w, b).mean\n",
    "tput = (m*n*k)*2/latency\n",
    "print(\"MatMul: %f ms, %.2f TFLOPS\" % (evaluator(a, w, b).mean * 1e3, tput/1e12))\n",
    "tvm.testing.assert_allclose(b.numpy(), np.matmul(a.numpy(), w.numpy()), rtol = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
