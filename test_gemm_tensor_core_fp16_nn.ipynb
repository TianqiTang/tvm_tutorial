{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating $B = A * W$, where both $A$ and $W$ are non-transposed (NN).\n",
    "\n",
    "Reference: https://zhuanlan.zhihu.com/p/410971069 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primfn(A_1: handle, W_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {B: Buffer(B_2: Pointer(float32), float32, [8192, 512], []),\n",
      "             A: Buffer(A_2: Pointer(float16), float16, [8192, 256], []),\n",
      "             W: Buffer(W_2: Pointer(float16), float16, [256, 512], [])}\n",
      "  buffer_map = {A_1: A, W_1: W, B_1: B} {\n",
      "  attr [IterVar(blockIdx.x: int32, (nullptr), \"ThreadIndex\", \"blockIdx.x\")] \"thread_extent\" = 32;\n",
      "  allocate(B.wmma.accumulator: Pointer(wmma.accumulator float32), float32, [4096]), storage_scope = wmma.accumulator;\n",
      "  allocate(A.shared: Pointer(shared float16), float16, [16384]), storage_scope = shared;\n",
      "  allocate(W.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;\n",
      "  allocate(A.shared.wmma.matrix_a: Pointer(wmma.matrix_a float16), float16, [1024]), storage_scope = wmma.matrix_a;\n",
      "  allocate(W.shared.wmma.matrix_b: Pointer(wmma.matrix_b float16), float16, [1024]), storage_scope = wmma.matrix_b;\n",
      "  attr [IterVar(blockIdx.y: int32, (nullptr), \"ThreadIndex\", \"blockIdx.y\")] \"thread_extent\" = 4;\n",
      "  attr [IterVar(threadIdx.y: int32, (nullptr), \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 8 {\n",
      "    for (h.c.outer.init: int32, 0, 4) {\n",
      "      for (w.c.outer.init: int32, 0, 4) {\n",
      "        @tir.tvm_fill_fragment(B.wmma.accumulator, 16, 16, 16, ((h.c.outer.init*4) + w.c.outer.init), 0f32, dtype=handle)\n",
      "      }\n",
      "    }\n",
      "    for (hdr.outer.outer: int32, 0, 4) {\n",
      "      for (ax0.ax1.fused.outer.outer: int32, 0, 8) {\n",
      "        attr [IterVar(threadIdx.x: int32, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 32;\n",
      "        A.shared[ramp((((ax0.ax1.fused.outer.outer*2048) + (threadIdx.y*256)) + (threadIdx.x*8)), 1, 8)] = (float16x8*)A_2[ramp(((((((blockIdx.x*65536) + (ax0.ax1.fused.outer.outer*8192)) + (threadIdx.y*1024)) + (floordiv(threadIdx.x, 8)*256)) + (hdr.outer.outer*64)) + (floormod(threadIdx.x, 8)*8)), 1, 8)]\n",
      "      }\n",
      "      for (ax0.ax1.fused.outer.outer_1: int32, 0, 4) {\n",
      "        attr [IterVar(threadIdx.x, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 32;\n",
      "        W.shared[ramp((((ax0.ax1.fused.outer.outer_1*2048) + (threadIdx.y*256)) + (threadIdx.x*8)), 1, 8)] = (float16x8*)W_2[ramp(((((((hdr.outer.outer*32768) + (ax0.ax1.fused.outer.outer_1*8192)) + (threadIdx.y*1024)) + (floordiv(threadIdx.x, 16)*512)) + (blockIdx.y*128)) + (floormod(threadIdx.x, 16)*8)), 1, 8)]\n",
      "      }\n",
      "      for (hdr.outer.inner: int32, 0, 4) {\n",
      "        for (ax0.outer: int32, 0, 4) {\n",
      "          @tir.tvm_load_matrix_sync(A.shared.wmma.matrix_a, 16, 16, 16, ax0.outer, @tir.tvm_access_ptr(@tir.type_annotation(, dtype=float16), A.shared, (((floordiv(threadIdx.y, 2)*4096) + (ax0.outer*1024)) + (hdr.outer.inner*16)), 1024, 1, dtype=handle), 64, \"row_major\", dtype=handle)\n",
      "        }\n",
      "        for (ax1.outer: int32, 0, 4) {\n",
      "          @tir.tvm_load_matrix_sync(W.shared.wmma.matrix_b, 16, 16, 16, ax1.outer, @tir.tvm_access_ptr(@tir.type_annotation(, dtype=float16), W.shared, (((hdr.outer.inner*2048) + (floormod(threadIdx.y, 2)*64)) + (ax1.outer*16)), 2048, 1, dtype=handle), 128, \"row_major\", dtype=handle)\n",
      "        }\n",
      "        for (h.c.outer: int32, 0, 4) {\n",
      "          for (w.c.outer: int32, 0, 4) {\n",
      "            @tir.tvm_mma_sync(B.wmma.accumulator, ((h.c.outer*4) + w.c.outer), A.shared.wmma.matrix_a, h.c.outer, W.shared.wmma.matrix_b, w.c.outer, B.wmma.accumulator, ((h.c.outer*4) + w.c.outer), dtype=handle)\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (h.inner.inner.outer: int32, 0, 4) {\n",
      "      for (w.inner.inner.outer: int32, 0, 4) {\n",
      "        @tir.tvm_store_matrix_sync(B.wmma.accumulator, 16, 16, 16, ((h.inner.inner.outer*4) + w.inner.inner.outer), @tir.tvm_access_ptr(@tir.type_annotation(, dtype=float32), B_2, ((((((blockIdx.x*131072) + (floordiv(threadIdx.y, 2)*32768)) + (h.inner.inner.outer*8192)) + (blockIdx.y*128)) + (floormod(threadIdx.y, 2)*64)) + (w.inner.inner.outer*16)), 8192, 2, dtype=handle), 512, \"row_major\", dtype=handle)\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)\n",
      "#include <cuda_fp16.h>\n",
      "__device__ half max(half a, half b)\n",
      "{\n",
      "  return __hgt(__half(a), __half(b)) ? a : b;\n",
      "}\n",
      "__device__ half min(half a, half b)\n",
      "{\n",
      "  return __hlt(__half(a), __half(b)) ? a : b;\n",
      "}\n",
      "#else\n",
      "\n",
      "typedef unsigned short uint16_t;\n",
      "typedef unsigned char uint8_t;\n",
      "typedef signed char int8_t;\n",
      "typedef int int32_t;\n",
      "typedef unsigned long long uint64_t;\n",
      "typedef unsigned int uint32_t;\n",
      "\n",
      "#define TVM_FORCE_INLINE inline __attribute__((always_inline))\n",
      "#define TVM_XINLINE TVM_FORCE_INLINE __device__ __host__\n",
      "#define TVM_ALIGNED(x) __attribute__ ((aligned(x)))\n",
      "#define TVM_HALF_OPERATOR(RTYPE, OP)                              \\\n",
      "  TVM_XINLINE RTYPE operator OP (half a, half b) {                \\\n",
      "    return RTYPE(float(a) OP float(b));                           \\\n",
      "  }                                                               \\\n",
      "  template<typename T>                                            \\\n",
      "  TVM_XINLINE RTYPE operator OP (half a, T b) {                   \\\n",
      "    return RTYPE(float(a) OP float(b));                           \\\n",
      "  }                                                               \\\n",
      "  template<typename T>                                            \\\n",
      "  TVM_XINLINE RTYPE operator OP (T a, half b) {                   \\\n",
      "    return RTYPE(float(a) OP float(b));                           \\\n",
      "  }\n",
      "\n",
      "#define TVM_HALF_ASSIGNOP(AOP, OP)                                \\\n",
      "  template<typename T>                                            \\\n",
      "  TVM_XINLINE half operator AOP (const T& a) {                    \\\n",
      "    return *this = half(float(*this) OP float(a));                \\\n",
      "  }                                                               \\\n",
      "  template<typename T>                                            \\\n",
      "  TVM_XINLINE half operator AOP (const volatile T& a) volatile {  \\\n",
      "    return *this = half(float(*this) OP float(a));                \\\n",
      "  }\n",
      "\n",
      "class TVM_ALIGNED(2) half {\n",
      " public:\n",
      "  uint16_t half_;\n",
      "\n",
      "  static TVM_XINLINE half Binary(uint16_t value) {\n",
      "    half res;\n",
      "    res.half_ = value;\n",
      "    return res;\n",
      "  }\n",
      "\n",
      "  TVM_XINLINE half() {}\n",
      "\n",
      "  TVM_XINLINE half(const float& value) { constructor(value); }\n",
      "  TVM_XINLINE explicit half(const double& value) { constructor(value); }\n",
      "  TVM_XINLINE explicit half(const int8_t& value) { constructor(value); }\n",
      "  TVM_XINLINE explicit half(const uint8_t& value) { constructor(value); }\n",
      "  TVM_XINLINE explicit half(const int32_t& value) { constructor(value); }\n",
      "  TVM_XINLINE explicit half(const uint32_t& value) { constructor(value); }\n",
      "  TVM_XINLINE explicit half(const long long& value) { constructor(value); }\n",
      "  TVM_XINLINE explicit half(const uint64_t& value) { constructor(value); }\n",
      "\n",
      "  TVM_XINLINE operator float() const {                          \\\n",
      "    return float(half2float(half_));                            \\\n",
      "  }                                                             \\\n",
      "  TVM_XINLINE operator float() const volatile {                 \\\n",
      "    return float(half2float(half_));                            \\\n",
      "  }\n",
      "\n",
      "\n",
      "  TVM_HALF_ASSIGNOP(+=, +)\n",
      "  TVM_HALF_ASSIGNOP(-=, -)\n",
      "  TVM_HALF_ASSIGNOP(*=, *)\n",
      "  TVM_HALF_ASSIGNOP(/=, /)\n",
      "\n",
      "  TVM_XINLINE half operator+() {\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  TVM_XINLINE half operator-() {\n",
      "    return half(-float(*this));\n",
      "  }\n",
      "\n",
      "  TVM_XINLINE half operator=(const half& a) {\n",
      "    half_ = a.half_;\n",
      "    return a;\n",
      "  }\n",
      "\n",
      "  template<typename T>\n",
      "  TVM_XINLINE half operator=(const T& a) {\n",
      "    return *this = half(a);\n",
      "  }\n",
      "\n",
      "  TVM_XINLINE half operator=(const half& a) volatile {\n",
      "    half_ = a.half_;\n",
      "    return a;\n",
      "  }\n",
      "\n",
      "  template<typename T>\n",
      "  TVM_XINLINE half operator=(const T& a) volatile {\n",
      "    return *this = half(a);\n",
      "  }\n",
      "\n",
      " private:\n",
      "  union Bits {\n",
      "    float f;\n",
      "    int32_t si;\n",
      "    uint32_t ui;\n",
      "  };\n",
      "\n",
      "  static int const fp16FractionBits = 10;\n",
      "  static int const fp32FractionBits = 23;\n",
      "  static int32_t const fp32FractionMask = ~(~0u << fp32FractionBits);   // == 0x7fffff\n",
      "  static int32_t const fp32HiddenBit = 1 << fp32FractionBits;   // == 0x800000\n",
      "  static int const shift = fp32FractionBits - fp16FractionBits;   // == 13\n",
      "  static int const shiftSign = 16;\n",
      "  static int32_t const expAdjust = 127 - 15;   // exp32-127 = exp16-15, so exp16 = exp32 - (127-15)\n",
      "\n",
      "  static int32_t const infN = 0x7F800000;   // flt32 infinity\n",
      "  static int32_t const maxN = 0x477FFFFF;   // max flt32 that's a flt16 normal after >> by shift\n",
      "  static int32_t const minN = 0x38800000;   // min flt16 normal as a flt32\n",
      "  static int32_t const maxZ = 0x33000000;   // max fp32 number that's still rounded to zero in fp16\n",
      "  static int32_t const signN = 0x80000000;  // flt32 sign bit\n",
      "\n",
      "  static int32_t const infC = infN >> shift;\n",
      "  static int32_t const nanN = (infC + 1) << shift;   // minimum flt16 nan as a flt32\n",
      "  static int32_t const maxC = maxN >> shift;\n",
      "  static int32_t const minC = minN >> shift;\n",
      "  static int32_t const signC = signN >> shiftSign;  // flt16 sign bit\n",
      "\n",
      "  static int32_t const mulN = 0x52000000;  // (1 << 23) / minN\n",
      "  static int32_t const mulC = 0x33800000;  // minN / (1 << (23 - shift))\n",
      "\n",
      "  static int32_t const subC = 0x003FF;  // max flt32 subnormal down shifted\n",
      "  static int32_t const norC = 0x00400;  // min flt32 normal down shifted\n",
      "\n",
      "  static int32_t const maxD = infC - maxC - 1;\n",
      "  static int32_t const minD = minC - subC - 1;\n",
      "\n",
      "  TVM_XINLINE uint16_t float2half(const float& value) const {\n",
      "    Bits v;\n",
      "    v.f = value;\n",
      "    uint32_t sign = v.si & signN;    // grab sign bit\n",
      "    v.si ^= sign;                    // clear sign bit from v\n",
      "    sign >>= shiftSign;              // logical shift sign to fp16 position\n",
      "\n",
      "    if (v.si <= maxZ) {\n",
      "      // Handle eventual zeros here to ensure\n",
      "      // vshift will not exceed 32 below.\n",
      "      v.ui = 0;\n",
      "    } else if (v.si < minN) {\n",
      "      // Handle denorms\n",
      "      uint32_t exp32 = v.ui >> fp32FractionBits;\n",
      "      int32_t exp16 = exp32 - expAdjust;\n",
      "      // If exp16 == 0 (just into the denorm range), then significant should be shifted right 1.\n",
      "      // Smaller (so negative) exp16 values should result in greater right shifts.\n",
      "      uint32_t vshift = 1 - exp16;\n",
      "      uint32_t significand = fp32HiddenBit | (v.ui & fp32FractionMask);\n",
      "      v.ui = significand >> vshift;\n",
      "      v.ui += (v.ui & 0x3fff) != 0x1000 || (significand & 0x7ff) ? 0x1000 : 0;\n",
      "    } else if (v.si <= maxN) {\n",
      "      // Handle norms\n",
      "      v.ui += (v.ui & 0x3fff) != 0x1000 ? 0x1000 : 0;\n",
      "      v.ui -= expAdjust << fp32FractionBits;\n",
      "    } else if (v.si <= infN) {\n",
      "      v.si = infN;\n",
      "    } else if (v.si < nanN) {\n",
      "      v.si = nanN;\n",
      "    }\n",
      "\n",
      "    v.ui >>= shift;\n",
      "    return sign | (v.ui & 0x7fff);\n",
      "  }\n",
      "\n",
      "  // Same as above routine, except for addition of volatile keyword\n",
      "  TVM_XINLINE uint16_t float2half(\n",
      "    const volatile float& value) const volatile {\n",
      "    Bits v;\n",
      "    v.f = value;\n",
      "    uint32_t sign = v.si & signN;    // grab sign bit\n",
      "    v.si ^= sign;                    // clear sign bit from v\n",
      "    sign >>= shiftSign;              // logical shift sign to fp16 position\n",
      "\n",
      "    if (v.si <= maxZ) {\n",
      "      // Handle eventual zeros here to ensure\n",
      "      // vshift will not exceed 32 below.\n",
      "      v.ui = 0;\n",
      "    } else if (v.si < minN) {\n",
      "      // Handle denorms\n",
      "      uint32_t exp32 = v.ui >> fp32FractionBits;\n",
      "      int32_t exp16 = exp32 - expAdjust;\n",
      "      // If exp16 == 0 (just into the denorm range), then significant should be shifted right 1.\n",
      "      // Smaller (so negative) exp16 values should result in greater right shifts.\n",
      "      uint32_t vshift = 1 - exp16;\n",
      "      uint32_t significand = fp32HiddenBit | (v.ui & fp32FractionMask);\n",
      "      v.ui = significand >> vshift;\n",
      "      v.ui += (v.ui & 0x3fff) != 0x1000 || (significand & 0x7ff) ? 0x1000 : 0;\n",
      "    } else if (v.si <= maxN) {\n",
      "      // Handle norms\n",
      "      v.ui += (v.ui & 0x3fff) != 0x1000 ? 0x1000 : 0;\n",
      "      v.ui -= expAdjust << fp32FractionBits;\n",
      "    } else if (v.si <= infN) {\n",
      "      v.si = infN;\n",
      "    } else if (v.si < nanN) {\n",
      "      v.si = nanN;\n",
      "    }\n",
      "\n",
      "    v.ui >>= shift;\n",
      "    return sign | (v.ui & 0x7fff);\n",
      "  }\n",
      "\n",
      "  TVM_XINLINE float half2float(const uint16_t& value) const {\n",
      "    Bits v;\n",
      "    v.ui = value;\n",
      "    int32_t sign = v.si & signC;\n",
      "    v.si ^= sign;\n",
      "    sign <<= shiftSign;\n",
      "    v.si ^= ((v.si + minD) ^ v.si) & -(v.si > subC);\n",
      "    v.si ^= ((v.si + maxD) ^ v.si) & -(v.si > maxC);\n",
      "    Bits s;\n",
      "    s.si = mulC;\n",
      "    s.f *= v.si;\n",
      "    int32_t mask = -(norC > v.si);\n",
      "    v.si <<= shift;\n",
      "    v.si ^= (s.si ^ v.si) & mask;\n",
      "    v.si |= sign;\n",
      "    return v.f;\n",
      "  }\n",
      "\n",
      "  TVM_XINLINE float half2float(\n",
      "    const volatile uint16_t& value) const volatile {\n",
      "    Bits v;\n",
      "    v.ui = value;\n",
      "    int32_t sign = v.si & signC;\n",
      "    v.si ^= sign;\n",
      "    sign <<= shiftSign;\n",
      "    v.si ^= ((v.si + minD) ^ v.si) & -(v.si > subC);\n",
      "    v.si ^= ((v.si + maxD) ^ v.si) & -(v.si > maxC);\n",
      "    Bits s;\n",
      "    s.si = mulC;\n",
      "    s.f *= v.si;\n",
      "    int32_t mask = -(norC > v.si);\n",
      "    v.si <<= shift;\n",
      "    v.si ^= (s.si ^ v.si) & mask;\n",
      "    v.si |= sign;\n",
      "    return v.f;\n",
      "  }\n",
      "\n",
      "  template<typename T>\n",
      "  TVM_XINLINE void constructor(const T& value) {\n",
      "    half_ = float2half(float(value));\n",
      "  }\n",
      "};\n",
      "\n",
      "TVM_HALF_OPERATOR(half, +)\n",
      "TVM_HALF_OPERATOR(half, -)\n",
      "TVM_HALF_OPERATOR(half, *)\n",
      "TVM_HALF_OPERATOR(half, /)\n",
      "TVM_HALF_OPERATOR(bool, >)\n",
      "TVM_HALF_OPERATOR(bool, <)\n",
      "TVM_HALF_OPERATOR(bool, >=)\n",
      "TVM_HALF_OPERATOR(bool, <=)\n",
      "\n",
      "TVM_XINLINE half __float2half_rn(const float a) {\n",
      "  return half(a);\n",
      "}\n",
      "#endif\n",
      "\n",
      "\n",
      "// Pack two half values.\n",
      "static inline __device__ __host__ unsigned\n",
      "__pack_half2(const half x, const half y) {\n",
      "  unsigned v0 = *((unsigned short *)&x);\n",
      "  unsigned v1 = *((unsigned short *)&y);\n",
      "  return (v1 << 16) | v0;\n",
      "}\n",
      "\n",
      "// fix undefined fp16 match function\n",
      "#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)\n",
      "static inline __device__ __host__ half hpow(half x, half y) {\n",
      "  float tmp_x = __half2float(x);\n",
      "  float tmp_y = __half2float(y);\n",
      "  float result = powf(tmp_x, tmp_y);\n",
      "  return __float2half(result);\n",
      "}\n",
      "\n",
      "static inline __device__ __host__ half htanh(half x) {\n",
      "  float tmp_x = __half2float(x);\n",
      "  float result = tanhf(tmp_x);\n",
      "  return __float2half(result);\n",
      "}\n",
      "#endif\n",
      "#include <mma.h>\n",
      "\n",
      "#ifdef _WIN32\n",
      "  using uint = unsigned int;\n",
      "  using uchar = unsigned char;\n",
      "  using ushort = unsigned short;\n",
      "  using int64_t = long long;\n",
      "  using uint64_t = unsigned long long;\n",
      "#else\n",
      "  #define uint unsigned int\n",
      "  #define uchar unsigned char\n",
      "  #define ushort unsigned short\n",
      "  #define int64_t long long\n",
      "  #define uint64_t unsigned long long\n",
      "#endif\n",
      "extern \"C\" __global__ void __launch_bounds__(256) default_function_kernel0(half* __restrict__ A, half* __restrict__ W, float* __restrict__ B) {\n",
      "  nvcuda::wmma::fragment<nvcuda::wmma::accumulator, 16, 16, 16, float> B_wmma_accumulator[16];\n",
      "  __shared__ half A_shared[16384];\n",
      "  __shared__ half W_shared[8192];\n",
      "  nvcuda::wmma::fragment<nvcuda::wmma::matrix_a, 16, 16, 16, half, nvcuda::wmma::row_major> A_shared_wmma_matrix_a[4];\n",
      "  nvcuda::wmma::fragment<nvcuda::wmma::matrix_b, 16, 16, 16, half, nvcuda::wmma::row_major> W_shared_wmma_matrix_b[4];\n",
      "  for (int h_c_outer_init = 0; h_c_outer_init < 4; ++h_c_outer_init) {\n",
      "    for (int w_c_outer_init = 0; w_c_outer_init < 4; ++w_c_outer_init) {\n",
      "      (void)nvcuda::wmma::fill_fragment(B_wmma_accumulator[((h_c_outer_init * 4) + w_c_outer_init)], 0.000000e+00f);\n",
      "    }\n",
      "  }\n",
      "  for (int hdr_outer_outer = 0; hdr_outer_outer < 4; ++hdr_outer_outer) {\n",
      "    __syncthreads();\n",
      "    for (int ax0_ax1_fused_outer_outer = 0; ax0_ax1_fused_outer_outer < 8; ++ax0_ax1_fused_outer_outer) {\n",
      "      ((uint4*)(A_shared + ((((ax0_ax1_fused_outer_outer * 2048) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.x) * 8)))))[0] = ((uint4*)(A + (((((((((int)blockIdx.x) * 65536) + (ax0_ax1_fused_outer_outer * 8192)) + (((int)threadIdx.y) * 1024)) + ((((int)threadIdx.x) >> 3) * 256)) + (hdr_outer_outer * 64)) + ((((int)threadIdx.x) & 7) * 8)))))[0];\n",
      "    }\n",
      "    for (int ax0_ax1_fused_outer_outer1 = 0; ax0_ax1_fused_outer_outer1 < 4; ++ax0_ax1_fused_outer_outer1) {\n",
      "      ((uint4*)(W_shared + ((((ax0_ax1_fused_outer_outer1 * 2048) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.x) * 8)))))[0] = ((uint4*)(W + (((((((hdr_outer_outer * 32768) + (ax0_ax1_fused_outer_outer1 * 8192)) + (((int)threadIdx.y) * 1024)) + ((((int)threadIdx.x) >> 4) * 512)) + (((int)blockIdx.y) * 128)) + ((((int)threadIdx.x) & 15) * 8)))))[0];\n",
      "    }\n",
      "    __syncthreads();\n",
      "    for (int hdr_outer_inner = 0; hdr_outer_inner < 4; ++hdr_outer_inner) {\n",
      "      for (int ax0_outer = 0; ax0_outer < 4; ++ax0_outer) {\n",
      "        (void)nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[ax0_outer], ((half *)A_shared + (((((((int)threadIdx.y) >> 1) * 4096) + (ax0_outer * 1024)) + (hdr_outer_inner * 16)))), 64);\n",
      "      }\n",
      "      for (int ax1_outer = 0; ax1_outer < 4; ++ax1_outer) {\n",
      "        (void)nvcuda::wmma::load_matrix_sync(W_shared_wmma_matrix_b[ax1_outer], ((half *)W_shared + ((((hdr_outer_inner * 2048) + ((((int)threadIdx.y) & 1) * 64)) + (ax1_outer * 16)))), 128);\n",
      "      }\n",
      "      for (int h_c_outer = 0; h_c_outer < 4; ++h_c_outer) {\n",
      "        for (int w_c_outer = 0; w_c_outer < 4; ++w_c_outer) {\n",
      "          (void)nvcuda::wmma::mma_sync(B_wmma_accumulator[((h_c_outer * 4) + w_c_outer)], A_shared_wmma_matrix_a[h_c_outer], W_shared_wmma_matrix_b[w_c_outer], B_wmma_accumulator[((h_c_outer * 4) + w_c_outer)]);\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  for (int h_inner_inner_outer = 0; h_inner_inner_outer < 4; ++h_inner_inner_outer) {\n",
      "    for (int w_inner_inner_outer = 0; w_inner_inner_outer < 4; ++w_inner_inner_outer) {\n",
      "      (void)nvcuda::wmma::store_matrix_sync(((float *)B + (((((((((int)blockIdx.x) * 131072) + ((((int)threadIdx.y) >> 1) * 32768)) + (h_inner_inner_outer * 8192)) + (((int)blockIdx.y) * 128)) + ((((int)threadIdx.y) & 1) * 64)) + (w_inner_inner_outer * 16)))), B_wmma_accumulator[((h_inner_inner_outer * 4) + w_inner_inner_outer)], 512, nvcuda::wmma::mem_row_major);\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time summary:\n",
      " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
      "   0.1346       0.1328       0.1440       0.1324       0.0040   \n",
      "               \n",
      "Matmul: 0.132542 ms\n"
     ]
    }
   ],
   "source": [
    "import tvm.testing\n",
    "import tvm\n",
    "from tvm import te\n",
    "import numpy as np\n",
    "\n",
    "n, hd, wd = 8192, 256, 512\n",
    "\n",
    "A = te.placeholder((n, hd), name=\"A\", dtype=\"float16\")\n",
    "W = te.placeholder((hd, wd), name=\"W\", dtype=\"float16\")\n",
    "\n",
    "hdr = te.reduce_axis((0, hd), \"hdr\")\n",
    "\n",
    "B = te.compute((n, wd), lambda h, w: te.sum(A[h, hdr].astype(\"float32\") * W[hdr, w].astype(\"float32\"), axis=hdr), name=\"B\")\n",
    "s = te.create_schedule(B.op)\n",
    "\n",
    "\n",
    "# Tensor Core support\n",
    "\n",
    "AS = s.cache_read(A, \"shared\", [B])\n",
    "WS = s.cache_read(W, \"shared\", [B])\n",
    "AF = s.cache_read(AS, \"wmma.matrix_a\", [B])\n",
    "WF = s.cache_read(WS, \"wmma.matrix_b\", [B])\n",
    "BF = s.cache_write(B, \"wmma.accumulator\")\n",
    "\n",
    "block_x = te.thread_axis(\"blockIdx.x\")\n",
    "block_y = te.thread_axis(\"blockIdx.y\")\n",
    "thread_x = te.thread_axis(\"threadIdx.x\")\n",
    "thread_y = te.thread_axis(\"threadIdx.y\")\n",
    "\n",
    "h, w = s[B].op.axis\n",
    "ho, hi = s[B].split(h, factor=256)\n",
    "wo, wi = s[B].split(w, factor=128)\n",
    "\n",
    "hio, hii = s[B].split(hi, 64)\n",
    "wio, wii = s[B].split(wi, 64)\n",
    "\n",
    "hiio, hiii = s[B].split(hii, 16)\n",
    "wiio, wiii = s[B].split(wii, 16)\n",
    "\n",
    "s[B].reorder(ho, wo, hio, wio, hiio, wiio, hiii, wiii)\n",
    "\n",
    "hwio = s[B].fuse(hio, wio)\n",
    "\n",
    "s[B].bind(ho, block_x)\n",
    "s[B].bind(wo, block_y)\n",
    "s[B].bind(hwio, thread_y)\n",
    "\n",
    "s[BF].compute_at(s[B], hwio)\n",
    "\n",
    "hbf, wbf = s[BF].op.axis\n",
    "hbfo, hbfi = s[BF].split(hbf, 16)\n",
    "wbfo, wbfi = s[BF].split(wbf, 16)\n",
    "\n",
    "(rbf,) = s[BF].op.reduce_axis\n",
    "rbfo, rbfi = s[BF].split(rbf, 16)\n",
    "\n",
    "# s[BF].reorder(rbfo, hbfo, wbfo, hbfi, wbfi, rbfi)\n",
    "\n",
    "rbfoo, rbfoi = s[BF].split(rbfo, 4)\n",
    "s[BF].reorder(rbfoo, rbfoi, hbfo, wbfo, hbfi, wbfi, rbfi)\n",
    "\n",
    "s[AF].compute_at(s[BF], rbfoi)\n",
    "s[WF].compute_at(s[BF], rbfoi)\n",
    "s[AS].compute_at(s[BF], rbfoo)\n",
    "s[WS].compute_at(s[BF], rbfoo)\n",
    "\n",
    "haf, waf = s[AF].op.axis\n",
    "hafo, hafi = s[AF].split(haf, 16)\n",
    "\n",
    "hwf, wwf = s[WF].op.axis\n",
    "wwfo, wwfi = s[WF].split(wwf, 16)\n",
    "s[WF].reorder(wwfo, hwf, wwfi)\n",
    "\n",
    "has, was = s[AS].op.axis\n",
    "hwas = s[AS].fuse(has, was)\n",
    "hwaso, hwasi = s[AS].split(hwas, 256)\n",
    "\n",
    "hws, wws = s[WS].op.axis\n",
    "hwws = s[WS].fuse(hws, wws)\n",
    "hwwso, hwwsi = s[WS].split(hwws, 256)\n",
    "\n",
    "hwasoo, hwasoi = s[AS].split(hwaso, 8)\n",
    "hwasio, hwasii = s[AS].split(hwasi, 8)\n",
    "\n",
    "hwwsoo, hwwsoi = s[WS].split(hwwso, 8)\n",
    "hwwsio, hwwsii = s[WS].split(hwwsi, 8)\n",
    "\n",
    "s[AS].bind(hwasoi, thread_y)\n",
    "s[AS].bind(hwasio, thread_x)\n",
    "s[AS].vectorize(hwasii)\n",
    "\n",
    "s[WS].bind(hwwsoi, thread_y)\n",
    "s[WS].bind(hwwsio, thread_x)\n",
    "s[WS].vectorize(hwwsii)\n",
    "\n",
    "def intrin_wmma_store_matrix():\n",
    "    n = 16\n",
    "    A = te.placeholder((n, n), name=\"A\", dtype=\"float32\")\n",
    "    BA = tvm.tir.decl_buffer(\n",
    "        A.shape, A.dtype, scope=\"wmma.accumulator\", data_alignment=32, offset_factor=1, strides=[64, 1]\n",
    "    )\n",
    "    C = te.compute((n, n), lambda i, j: A[i, j], name=\"C\")\n",
    "    BC = tvm.tir.decl_buffer(C.shape, C.dtype, scope=\"global\", data_alignment=32, offset_factor=1, strides=[wd, 1])\n",
    "\n",
    "    def intrin_func(ins, outs):\n",
    "        ib = tvm.tir.ir_builder.create()\n",
    "        BA = ins[0]\n",
    "        BC = outs[0]\n",
    "        ib.emit(\n",
    "            tvm.tir.call_intrin(\n",
    "                \"handle\",\n",
    "                \"tir.tvm_store_matrix_sync\",\n",
    "                BA.data,\n",
    "                n,\n",
    "                n,\n",
    "                n,\n",
    "                BA.elem_offset//1024*4 + (BA.elem_offset//16) % 4,\n",
    "                BC.access_ptr(\"w\"),\n",
    "                wd,\n",
    "                \"row_major\",\n",
    "            )\n",
    "        )\n",
    "        return ib.get()\n",
    "\n",
    "    return te.decl_tensor_intrin(C.op, intrin_func, binds={A: BA, C: BC})\n",
    "\n",
    "s[B].tensorize(hiii, intrin_wmma_store_matrix())\n",
    "\n",
    "def intrin_wmma_gemm():\n",
    "    n = 16\n",
    "    A = te.placeholder((n, n), name=\"A\", dtype=\"float16\")\n",
    "    B = te.placeholder((n, n), name=\"B\", dtype=\"float16\")\n",
    "    k = te.reduce_axis((0, n), name=\"k\")\n",
    "    C = te.compute(\n",
    "        (n, n),\n",
    "        lambda ii, jj: te.sum(A[ii, k].astype(\"float\") * B[k, jj].astype(\"float\"), axis=k),\n",
    "        name=\"C\",\n",
    "    )\n",
    "    BA = tvm.tir.decl_buffer(\n",
    "        A.shape, A.dtype, name=\"BA\", scope=\"wmma.matrix_a\", data_alignment=32, offset_factor=256, strides=[16, 1]\n",
    "    )\n",
    "    BB = tvm.tir.decl_buffer(\n",
    "        B.shape, B.dtype, name=\"BB\", scope=\"wmma.matrix_b\", data_alignment=32, offset_factor=16, strides=[64, 1]\n",
    "    )\n",
    "    BC = tvm.tir.decl_buffer(\n",
    "        C.shape, C.dtype, name=\"BC\", scope=\"wmma.accumulator\", data_alignment=32, offset_factor=1, strides=[64, 1]\n",
    "    )\n",
    "\n",
    "    def intrin_func(ins, outs):\n",
    "        BA, BB = ins\n",
    "        (BC,) = outs\n",
    "\n",
    "        def init():\n",
    "            ib = tvm.tir.ir_builder.create()\n",
    "            ib.emit(\n",
    "                tvm.tir.call_intrin(\n",
    "                    \"handle\", \"tir.tvm_fill_fragment\", \n",
    "                    BC.data, n, n, n, BC.elem_offset//1024*4+(BC.elem_offset//16)%4, 0.0\n",
    "                )\n",
    "            )\n",
    "            return ib.get()\n",
    "\n",
    "        def update():\n",
    "            ib = tvm.tir.ir_builder.create()\n",
    "            ib.emit(\n",
    "                tvm.tir.call_intrin(\n",
    "                    \"handle\",\n",
    "                    \"tir.tvm_mma_sync\",\n",
    "                    BC.data,\n",
    "                    BC.elem_offset//1024*4+(BC.elem_offset//16)%4,\n",
    "                    BA.data,\n",
    "                    BA.elem_offset//256,\n",
    "                    BB.data,\n",
    "                    BB.elem_offset//16,\n",
    "                    BC.data,\n",
    "                    BC.elem_offset//1024*4+(BC.elem_offset//16)%4,\n",
    "                )\n",
    "            )\n",
    "            return ib.get()\n",
    "\n",
    "        return update(), init(), update()\n",
    "\n",
    "    return te.decl_tensor_intrin(C.op, intrin_func, binds={A: BA, B: BB, C: BC})\n",
    "\n",
    "s[BF].tensorize(hbfi, intrin_wmma_gemm())\n",
    "\n",
    "def intrin_wmma_load_matrix_a(scope):\n",
    "    n = 16\n",
    "    A = te.placeholder((n, n), name=\"A\", dtype=\"float16\")\n",
    "    BA = tvm.tir.decl_buffer(A.shape, A.dtype, scope=\"shared\", data_alignment=32, offset_factor=1, strides=[64, 1])\n",
    "    C = te.compute((n, n), lambda i, j: A[i, j], name=\"C\")\n",
    "    BC = tvm.tir.decl_buffer(C.shape, C.dtype, scope=scope, data_alignment=32, offset_factor=1, strides=[16, 1])\n",
    "\n",
    "\n",
    "    def intrin_func(ins, outs):\n",
    "        ib = tvm.tir.ir_builder.create()\n",
    "\n",
    "        BA = ins[0]\n",
    "        BC = outs[0]\n",
    "        ib.emit(\n",
    "            tvm.tir.call_intrin(\n",
    "                \"handle\",\n",
    "                \"tir.tvm_load_matrix_sync\",\n",
    "                BC.data,\n",
    "                n,\n",
    "                n,\n",
    "                n,\n",
    "                BC.elem_offset//256,\n",
    "                BA.access_ptr(\"r\"),\n",
    "                64,\n",
    "                \"row_major\",\n",
    "            )\n",
    "        )\n",
    "        return ib.get()\n",
    "\n",
    "    return te.decl_tensor_intrin(C.op, intrin_func, binds={A: BA, C: BC})\n",
    "\n",
    "\n",
    "def intrin_wmma_load_matrix_b(scope):\n",
    "    n = 16\n",
    "    A = te.placeholder((n, n), name=\"A\", dtype=\"float16\")\n",
    "    BA = tvm.tir.decl_buffer(A.shape, A.dtype, scope=\"shared\", data_alignment=32, offset_factor=1, strides=[128, 1])\n",
    "    C = te.compute((n, n), lambda i, j: A[i, j], name=\"C\")\n",
    "    BC = tvm.tir.decl_buffer(C.shape, C.dtype, scope=scope, data_alignment=32, offset_factor=1, strides=[64, 1])\n",
    "\n",
    "    def intrin_func(ins, outs):\n",
    "        ib = tvm.tir.ir_builder.create()\n",
    "\n",
    "        BA = ins[0]\n",
    "        BC = outs[0]\n",
    "        ib.emit(\n",
    "            tvm.tir.call_intrin(\n",
    "                \"handle\",\n",
    "                \"tir.tvm_load_matrix_sync\",\n",
    "                BC.data,\n",
    "                n,\n",
    "                n,\n",
    "                n,\n",
    "                BC.elem_offset//16,\n",
    "                BA.access_ptr(\"r\"),\n",
    "                128,\n",
    "                \"row_major\",\n",
    "            )\n",
    "        )\n",
    "        return ib.get()\n",
    "\n",
    "    return te.decl_tensor_intrin(C.op, intrin_func, binds={A: BA, C: BC})\n",
    "\n",
    "s[AF].tensorize(hafi, intrin_wmma_load_matrix_a(\"wmma.matrix_a\"))\n",
    "s[WF].tensorize(hwf, intrin_wmma_load_matrix_b(\"wmma.matrix_b\"))\n",
    "\n",
    "print(tvm.lower(s, [A, W, B], simple_mode=True))\n",
    "\n",
    "dev = tvm.cuda(0)\n",
    "func = tvm.build(s, [A, W, B], \"cuda\")\n",
    "\n",
    "print(func.imported_modules[0].get_source())\n",
    "\n",
    "a_np = np.random.uniform(size=(n, hd)).astype(A.dtype)\n",
    "w_np = np.random.uniform(size=(hd, wd)).astype(W.dtype)\n",
    "\n",
    "a = tvm.nd.array(a_np, dev)\n",
    "w = tvm.nd.array(w_np, dev)\n",
    "\n",
    "b = tvm.nd.array(np.zeros((n, wd), dtype=B.dtype), dev)\n",
    "\n",
    "func(a, w, b)\n",
    "\n",
    "evaluator = func.time_evaluator(func.entry_name, dev, repeat=20, number=300)\n",
    "print(evaluator(a, w, b))\n",
    "print(\"Matmul: %f ms\" % (evaluator(a, w, b).mean * 1e3))\n",
    "\n",
    "tvm.testing.assert_allclose(b.numpy(), np.matmul(a.numpy(), w.numpy()), rtol = 0.001)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
