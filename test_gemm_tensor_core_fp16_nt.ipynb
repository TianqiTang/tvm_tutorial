{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating $B = A * W^T$, where \n",
    "- $A$ is non-transposed and $W$ is transposed (NT).\n",
    "- $A$ and $W$ are both fp16, while $B$ is fp32.\n",
    "\n",
    "Reference: https://zhuanlan.zhihu.com/p/410971069 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import te\n",
    "n, hd, wd = 8192, 256, 512\n",
    "\n",
    "A = te.placeholder((n, hd), name=\"A\", dtype=\"float16\")\n",
    "W = te.placeholder((wd, hd), name=\"W\", dtype=\"float16\")\n",
    "hdr = te.reduce_axis((0, hd), \"hdr\")\n",
    "B = te.compute((n, wd), lambda h, w: te.sum(A[h, hdr].astype(\"float32\") * W[w, hdr].astype(\"float32\"), axis=hdr), name=\"B\")\n",
    "\n",
    "s = te.create_schedule(B.op)\n",
    "AS = s.cache_read(A, \"shared\", [B])\n",
    "WS = s.cache_read(W, \"shared\", [B])\n",
    "AF = s.cache_read(AS, \"wmma.matrix_a\", [B])\n",
    "WF = s.cache_read(WS, \"wmma.matrix_b\", [B])\n",
    "BF = s.cache_write(B, \"wmma.accumulator\")\n",
    "\n",
    "block_x = te.thread_axis(\"blockIdx.x\")\n",
    "block_y = te.thread_axis(\"blockIdx.y\")\n",
    "thread_x = te.thread_axis(\"threadIdx.x\")\n",
    "thread_y = te.thread_axis(\"threadIdx.y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = s[B].op.axis\n",
    "ho, hi = s[B].split(h, factor=256)\n",
    "wo, wi = s[B].split(w, factor=128)\n",
    "hio, hii = s[B].split(hi, 64)\n",
    "wio, wii = s[B].split(wi, 64)\n",
    "hiio, hiii = s[B].split(hii, 16)\n",
    "wiio, wiii = s[B].split(wii, 16)\n",
    "s[B].reorder(ho, wo, hio, wio, hiio, wiio, hiii, wiii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hwio = s[B].fuse(hio, wio)\n",
    "s[B].bind(ho, block_x)\n",
    "s[B].bind(wo, block_y)\n",
    "s[B].bind(hwio, thread_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[BF].compute_at(s[B], hwio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbf, wbf = s[BF].op.axis\n",
    "hbfo, hbfi = s[BF].split(hbf, 16)\n",
    "wbfo, wbfi = s[BF].split(wbf, 16)\n",
    "\n",
    "(rbf,) = s[BF].op.reduce_axis\n",
    "rbfo, rbfi = s[BF].split(rbf, 16)\n",
    "s[BF].reorder(rbfo, hbfo, wbfo, hbfi, wbfi, rbfi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbfoo, rbfoi = s[BF].split(rbfo, 4)\n",
    "\n",
    "# s[BF].reorder(rbfoo, rbfoi, hbfo, wbfo, hbfi, wbfi, rbfi)\n",
    "\n",
    "s[AF].compute_at(s[BF], rbfoi)\n",
    "s[WF].compute_at(s[BF], rbfoi)\n",
    "s[AS].compute_at(s[BF], rbfoo)\n",
    "s[WS].compute_at(s[BF], rbfoo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "haf, waf = s[AF].op.axis\n",
    "hafo, hafi = s[AF].split(haf, 16)\n",
    "\n",
    "hwf, wwf = s[WF].op.axis\n",
    "hwfo, hwfi = s[WF].split(hwf, 16)\n",
    "s[WF].reorder(hwfo, wwf, hwfi)\n",
    "# wwfo, wwfi = s[WF].split(wwf, 16)\n",
    "# s[WF].reorder(wwfo, hwf, wwfi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "has, was = s[AS].op.axis\n",
    "hwas = s[AS].fuse(has, was)\n",
    "hwaso, hwasi = s[AS].split(hwas, 256)\n",
    "\n",
    "hws, wws = s[WS].op.axis\n",
    "hwws = s[WS].fuse(hws, wws)\n",
    "hwwso, hwwsi = s[WS].split(hwws, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hwasoo, hwasoi = s[AS].split(hwaso, 8)\n",
    "hwasio, hwasii = s[AS].split(hwasi, 8)\n",
    "\n",
    "hwwsoo, hwwsoi = s[WS].split(hwwso, 8)\n",
    "hwwsio, hwwsii = s[WS].split(hwwsi, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[AS].bind(hwasoi, thread_y)\n",
    "s[AS].bind(hwasio, thread_x)\n",
    "s[AS].vectorize(hwasii)\n",
    "\n",
    "s[WS].bind(hwwsoi, thread_y)\n",
    "s[WS].bind(hwwsio, thread_x)\n",
    "s[WS].vectorize(hwwsii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intrin_wmma_store_matrix():\n",
    "    n = 16\n",
    "    # Implement the compute pattern with te\n",
    "    A = te.placeholder((n, n), name=\"A\", dtype=\"float32\")\n",
    "    BA = tvm.tir.decl_buffer(\n",
    "        A.shape, \n",
    "        A.dtype, \n",
    "        scope=\"wmma.accumulator\", \n",
    "        data_alignment=32,    # in unit of \"Byte\" \n",
    "        offset_factor=1, \n",
    "        strides=[64, 1]\n",
    "    )\n",
    "    C = te.compute((n, n), lambda i, j: A[i, j], name=\"C\")\n",
    "    BC = tvm.tir.decl_buffer(\n",
    "        C.shape, \n",
    "        C.dtype, \n",
    "        scope=\"global\", \n",
    "        data_alignment=32, \n",
    "        offset_factor=1, \n",
    "        strides=[wd, 1]\n",
    "    )\n",
    "\n",
    "    def intrin_func(ins, outs):\n",
    "        ib = tvm.tir.ir_builder.create()\n",
    "        BA = ins[0]\n",
    "        BC = outs[0]\n",
    "        ib.emit(\n",
    "            # source code: tvm/src/target/source/codegen_cuda.cc\n",
    "            # translated in to cpp code:\n",
    "            # nvcuda::wmma::store_matrix_sync(\n",
    "            #     BC_ptr, \n",
    "            #     BA_data[BA_element_offset], \n",
    "            #     wd, \n",
    "            #     nvcuda::wmma::mem_row_major\n",
    "            # )\n",
    "            tvm.tir.call_intrin(\n",
    "                \"handle\",\n",
    "                \"tir.tvm_store_matrix_sync\",\n",
    "                BA.data,    # op->args[0]\n",
    "                n, n, n,    # op->args[1,2,3]\n",
    "                BA.elem_offset//1024 * 4 + (BA.elem_offset//16) % 4,\n",
    "                BC.access_ptr(\"w\"),    # op->args[5]: w=write\n",
    "                wd,         # op->args[6]\n",
    "                \"row_major\",# op->args[7]\n",
    "            )\n",
    "        )\n",
    "        return ib.get()\n",
    "    return te.decl_tensor_intrin(C.op, intrin_func, binds={A: BA, C: BC})\n",
    "\n",
    "s[B].tensorize(hiii, intrin_wmma_store_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primfn(A_1: handle, W_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {B: Buffer(B_2: Pointer(float32), float32, [8192, 512], []),\n",
      "             A: Buffer(A_2: Pointer(float16), float16, [8192, 256], []),\n",
      "             W: Buffer(W_2: Pointer(float16), float16, [512, 256], [])}\n",
      "  buffer_map = {A_1: A, W_1: W, B_1: B} {\n",
      "  attr [IterVar(blockIdx.x: int32, (nullptr), \"ThreadIndex\", \"blockIdx.x\")] \"thread_extent\" = 32;\n",
      "  allocate(B.wmma.accumulator: Pointer(wmma.accumulator float32), float32, [4096]), storage_scope = wmma.accumulator;\n",
      "  allocate(A.shared: Pointer(shared float16), float16, [16384]), storage_scope = shared;\n",
      "  allocate(W.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;\n",
      "  allocate(A.shared.wmma.matrix_a: Pointer(wmma.matrix_a float16), float16, [1024]), storage_scope = wmma.matrix_a;\n",
      "  allocate(W.shared.wmma.matrix_b: Pointer(wmma.matrix_b float16), float16, [1024]), storage_scope = wmma.matrix_b;\n",
      "  attr [IterVar(blockIdx.y: int32, (nullptr), \"ThreadIndex\", \"blockIdx.y\")] \"thread_extent\" = 4;\n",
      "  attr [IterVar(threadIdx.y: int32, (nullptr), \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 8 {\n",
      "    for (h.c.outer.init: int32, 0, 4) {\n",
      "      for (w.c.outer.init: int32, 0, 4) {\n",
      "        for (h.c.inner.init: int32, 0, 16) {\n",
      "          for (w.c.inner.init: int32, 0, 16) {\n",
      "            B.wmma.accumulator[((((h.c.outer.init*1024) + (h.c.inner.init*64)) + (w.c.outer.init*16)) + w.c.inner.init)] = 0f32\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (hdr.outer.outer: int32, 0, 4) {\n",
      "      for (ax0.ax1.fused.outer.outer: int32, 0, 8) {\n",
      "        attr [IterVar(threadIdx.x: int32, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 32;\n",
      "        A.shared[ramp((((ax0.ax1.fused.outer.outer*2048) + (threadIdx.y*256)) + (threadIdx.x*8)), 1, 8)] = (float16x8*)A_2[ramp(((((((blockIdx.x*65536) + (ax0.ax1.fused.outer.outer*8192)) + (threadIdx.y*1024)) + (floordiv(threadIdx.x, 8)*256)) + (hdr.outer.outer*64)) + (floormod(threadIdx.x, 8)*8)), 1, 8)]\n",
      "      }\n",
      "      for (ax0.ax1.fused.outer.outer_1: int32, 0, 4) {\n",
      "        attr [IterVar(threadIdx.x, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 32;\n",
      "        W.shared[ramp((((ax0.ax1.fused.outer.outer_1*2048) + (threadIdx.y*256)) + (threadIdx.x*8)), 1, 8)] = (float16x8*)W_2[ramp(((((((blockIdx.y*32768) + (ax0.ax1.fused.outer.outer_1*8192)) + (threadIdx.y*1024)) + (floordiv(threadIdx.x, 8)*256)) + (hdr.outer.outer*64)) + (floormod(threadIdx.x, 8)*8)), 1, 8)]\n",
      "      }\n",
      "      for (hdr.outer.inner: int32, 0, 4) {\n",
      "        for (ax0.outer: int32, 0, 4) {\n",
      "          for (ax0.inner: int32, 0, 16) {\n",
      "            for (ax1: int32, 0, 16) {\n",
      "              A.shared.wmma.matrix_a[(((ax0.outer*256) + (ax0.inner*16)) + ax1)] = (float16*)A.shared[(((((floordiv(threadIdx.y, 2)*4096) + (ax0.outer*1024)) + (ax0.inner*64)) + (hdr.outer.inner*16)) + ax1)]\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        for (ax0.outer_1: int32, 0, 4) {\n",
      "          for (ax1_1: int32, 0, 16) {\n",
      "            for (ax0.inner_1: int32, 0, 16) {\n",
      "              W.shared.wmma.matrix_b[(((ax0.outer_1*256) + (ax0.inner_1*16)) + ax1_1)] = (float16*)W.shared[(((((floormod(threadIdx.y, 2)*4096) + (ax0.outer_1*1024)) + (ax0.inner_1*64)) + (hdr.outer.inner*16)) + ax1_1)]\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        for (h.c.outer: int32, 0, 4) {\n",
      "          for (w.c.outer: int32, 0, 4) {\n",
      "            for (h.c.inner: int32, 0, 16) {\n",
      "              for (w.c.inner: int32, 0, 16) {\n",
      "                for (hdr.inner: int32, 0, 16) {\n",
      "                  B.wmma.accumulator[((((h.c.outer*1024) + (h.c.inner*64)) + (w.c.outer*16)) + w.c.inner)] = ((float32*)B.wmma.accumulator[((((h.c.outer*1024) + (h.c.inner*64)) + (w.c.outer*16)) + w.c.inner)] + (cast(float32, (float16*)A.shared.wmma.matrix_a[(((h.c.outer*256) + (h.c.inner*16)) + hdr.inner)])*cast(float32, (float16*)W.shared.wmma.matrix_b[(((w.c.outer*256) + (w.c.inner*16)) + hdr.inner)])))\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (h.inner.inner.outer: int32, 0, 4) {\n",
      "      for (w.inner.inner.outer: int32, 0, 4) {\n",
      "        @tir.tvm_store_matrix_sync(B.wmma.accumulator, 16, 16, 16, ((h.inner.inner.outer*4) + w.inner.inner.outer), @tir.tvm_access_ptr(@tir.type_annotation(, dtype=float32), B_2, ((((((blockIdx.x*131072) + (floordiv(threadIdx.y, 2)*32768)) + (h.inner.inner.outer*8192)) + (blockIdx.y*128)) + (floormod(threadIdx.y, 2)*64)) + (w.inner.inner.outer*16)), 8192, 2, dtype=handle), 512, \"row_major\", dtype=handle)\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.lower(s, [A, W, B], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intrin_wmma_gemm():\n",
    "    n = 16\n",
    "    # Implement the compute pattern with te\n",
    "    A = te.placeholder((n, n), name=\"A\", dtype=\"float16\")\n",
    "    B = te.placeholder((n, n), name=\"B\", dtype=\"float16\")\n",
    "    k = te.reduce_axis((0, n), name=\"k\")\n",
    "    C = te.compute(\n",
    "        (n, n),\n",
    "        lambda ii, jj: te.sum(A[ii, k].astype(\"float\") * B[jj, k].astype(\"float\"), axis=k), name=\"C\"\n",
    "    )\n",
    "    BA = tvm.tir.decl_buffer(\n",
    "        A.shape, A.dtype, name=\"BA\", scope=\"wmma.matrix_a\", data_alignment=32, offset_factor=1, strides=[16, 1]\n",
    "    )\n",
    "    BB = tvm.tir.decl_buffer(\n",
    "        B.shape, B.dtype, name=\"BB\", scope=\"wmma.matrix_b\", data_alignment=32, offset_factor=1, strides=[16, 1]\n",
    "    )\n",
    "    BC = tvm.tir.decl_buffer(\n",
    "        C.shape, C.dtype, name=\"BC\", scope=\"wmma.accumulator\", data_alignment=32, offset_factor=1, strides=[64, 1]\n",
    "    )\n",
    "\n",
    "    def intrin_func(ins, outs):\n",
    "        BA, BB = ins\n",
    "        (BC, ) = outs\n",
    "\n",
    "        def init():\n",
    "            ib = tvm.tir.ir_builder.create()\n",
    "            ib.emit(\n",
    "                tvm.tir.call_intrin(\n",
    "                    \"handle\", \"tir.tvm_fill_fragment\", BC.data, n, n, n, \n",
    "                    BC.elem_offset//1024*4 + (BC.elem_offset//16) % 4,\n",
    "                    0.0\n",
    "                )\n",
    "            )\n",
    "            return ib.get()\n",
    "        \n",
    "        def update():\n",
    "            ib = tvm.tir.ir_builder.create()\n",
    "            ib.emit(\n",
    "                tvm.tir.call_intrin(\n",
    "                    \"handle\", \"tir.tvm_mma_sync\",\n",
    "                    BC.data,\n",
    "                    BC.elem_offset//1024 * 4 + (BC.elem_offset//16)%4,\n",
    "                    BA.data,\n",
    "                    BA.elem_offset//256,\n",
    "                    BB.data,\n",
    "                    BB.elem_offset//256,\n",
    "                    BC.data,\n",
    "                    BC.elem_offset//1024 * 4 + (BC.elem_offset//16)%4,\n",
    "                )\n",
    "            )\n",
    "            return ib.get()\n",
    "        \n",
    "        return update(), init(), update()\n",
    "    \n",
    "    return te.decl_tensor_intrin(C.op, intrin_func, binds={A: BA, B: BB, C: BC})\n",
    "\n",
    "s[BF].tensorize(hbfi, intrin_wmma_gemm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primfn(A_1: handle, W_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {B: Buffer(B_2: Pointer(float32), float32, [8192, 512], []),\n",
      "             A: Buffer(A_2: Pointer(float16), float16, [8192, 256], []),\n",
      "             W: Buffer(W_2: Pointer(float16), float16, [512, 256], [])}\n",
      "  buffer_map = {A_1: A, W_1: W, B_1: B} {\n",
      "  attr [IterVar(blockIdx.x: int32, (nullptr), \"ThreadIndex\", \"blockIdx.x\")] \"thread_extent\" = 32;\n",
      "  allocate(B.wmma.accumulator: Pointer(wmma.accumulator float32), float32, [4096]), storage_scope = wmma.accumulator;\n",
      "  allocate(A.shared: Pointer(shared float16), float16, [16384]), storage_scope = shared;\n",
      "  allocate(W.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;\n",
      "  allocate(A.shared.wmma.matrix_a: Pointer(wmma.matrix_a float16), float16, [1024]), storage_scope = wmma.matrix_a;\n",
      "  allocate(W.shared.wmma.matrix_b: Pointer(wmma.matrix_b float16), float16, [1024]), storage_scope = wmma.matrix_b;\n",
      "  attr [IterVar(blockIdx.y: int32, (nullptr), \"ThreadIndex\", \"blockIdx.y\")] \"thread_extent\" = 4;\n",
      "  attr [IterVar(threadIdx.y: int32, (nullptr), \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 8 {\n",
      "    for (h.c.outer.init: int32, 0, 4) {\n",
      "      for (w.c.outer.init: int32, 0, 4) {\n",
      "        @tir.tvm_fill_fragment(B.wmma.accumulator, 16, 16, 16, ((h.c.outer.init*4) + w.c.outer.init), 0f32, dtype=handle)\n",
      "      }\n",
      "    }\n",
      "    for (hdr.outer.outer: int32, 0, 4) {\n",
      "      for (ax0.ax1.fused.outer.outer: int32, 0, 8) {\n",
      "        attr [IterVar(threadIdx.x: int32, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 32;\n",
      "        A.shared[ramp((((ax0.ax1.fused.outer.outer*2048) + (threadIdx.y*256)) + (threadIdx.x*8)), 1, 8)] = (float16x8*)A_2[ramp(((((((blockIdx.x*65536) + (ax0.ax1.fused.outer.outer*8192)) + (threadIdx.y*1024)) + (floordiv(threadIdx.x, 8)*256)) + (hdr.outer.outer*64)) + (floormod(threadIdx.x, 8)*8)), 1, 8)]\n",
      "      }\n",
      "      for (ax0.ax1.fused.outer.outer_1: int32, 0, 4) {\n",
      "        attr [IterVar(threadIdx.x, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 32;\n",
      "        W.shared[ramp((((ax0.ax1.fused.outer.outer_1*2048) + (threadIdx.y*256)) + (threadIdx.x*8)), 1, 8)] = (float16x8*)W_2[ramp(((((((blockIdx.y*32768) + (ax0.ax1.fused.outer.outer_1*8192)) + (threadIdx.y*1024)) + (floordiv(threadIdx.x, 8)*256)) + (hdr.outer.outer*64)) + (floormod(threadIdx.x, 8)*8)), 1, 8)]\n",
      "      }\n",
      "      for (hdr.outer.inner: int32, 0, 4) {\n",
      "        for (ax0.outer: int32, 0, 4) {\n",
      "          for (ax0.inner: int32, 0, 16) {\n",
      "            for (ax1: int32, 0, 16) {\n",
      "              A.shared.wmma.matrix_a[(((ax0.outer*256) + (ax0.inner*16)) + ax1)] = (float16*)A.shared[(((((floordiv(threadIdx.y, 2)*4096) + (ax0.outer*1024)) + (ax0.inner*64)) + (hdr.outer.inner*16)) + ax1)]\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        for (ax0.outer_1: int32, 0, 4) {\n",
      "          for (ax1_1: int32, 0, 16) {\n",
      "            for (ax0.inner_1: int32, 0, 16) {\n",
      "              W.shared.wmma.matrix_b[(((ax0.outer_1*256) + (ax0.inner_1*16)) + ax1_1)] = (float16*)W.shared[(((((floormod(threadIdx.y, 2)*4096) + (ax0.outer_1*1024)) + (ax0.inner_1*64)) + (hdr.outer.inner*16)) + ax1_1)]\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        for (h.c.outer: int32, 0, 4) {\n",
      "          for (w.c.outer: int32, 0, 4) {\n",
      "            @tir.tvm_mma_sync(B.wmma.accumulator, ((h.c.outer*4) + w.c.outer), A.shared.wmma.matrix_a, h.c.outer, W.shared.wmma.matrix_b, w.c.outer, B.wmma.accumulator, ((h.c.outer*4) + w.c.outer), dtype=handle)\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (h.inner.inner.outer: int32, 0, 4) {\n",
      "      for (w.inner.inner.outer: int32, 0, 4) {\n",
      "        @tir.tvm_store_matrix_sync(B.wmma.accumulator, 16, 16, 16, ((h.inner.inner.outer*4) + w.inner.inner.outer), @tir.tvm_access_ptr(@tir.type_annotation(, dtype=float32), B_2, ((((((blockIdx.x*131072) + (floordiv(threadIdx.y, 2)*32768)) + (h.inner.inner.outer*8192)) + (blockIdx.y*128)) + (floormod(threadIdx.y, 2)*64)) + (w.inner.inner.outer*16)), 8192, 2, dtype=handle), 512, \"row_major\", dtype=handle)\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.lower(s, [A, W, B], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intrin_wmma_load_matrix_a(scope):\n",
    "    n = 16\n",
    "    A = te.placeholder((n, n), name=\"A\", dtype=\"float16\")\n",
    "    BA = tvm.tir.decl_buffer(A.shape, A.dtype, scope=\"shared\", data_alignment=32, offset_factor=1, strides=[64, 1])\n",
    "    C = te.compute((n, n), lambda i, j: A[i, j], name=\"C\")\n",
    "    BC = tvm.tir.decl_buffer(C.shape, C.dtype, scope=scope, data_alignment=32, offset_factor=1, strides=[16, 1])\n",
    "\n",
    "    def intrin_func(ins, outs):\n",
    "        ib = tvm.tir.ir_builder.create()\n",
    "        BA = ins[0]\n",
    "        BC = outs[0]\n",
    "        ib.emit(\n",
    "            tvm.tir.call_intrin(\n",
    "                \"handle\", \"tir.tvm_load_matrix_sync\",\n",
    "                BC.data, n, n, n,\n",
    "                BC.elem_offset // 256,\n",
    "                BA.access_ptr(\"r\"),\n",
    "                64,\n",
    "                \"row_major\"\n",
    "            )\n",
    "        )\n",
    "        return ib.get()\n",
    "    return te.decl_tensor_intrin(C.op, intrin_func, binds={A: BA, C: BC})\n",
    "\n",
    "def intrin_wmma_load_matrix_b(scope):\n",
    "    n = 16\n",
    "    A = te.placeholder((n, n), name=\"A\", dtype=\"float16\")\n",
    "    BA = tvm.tir.decl_buffer(A.shape, A.dtype, scope=\"shared\", data_alignment=32, offset_factor=1, strides=[64, 1])\n",
    "    C = te.compute((n, n), lambda i, j: A[i, j], name=\"C\")\n",
    "    BC = tvm.tir.decl_buffer(C.shape, C.dtype, scope=scope, data_alignment=32, offset_factor=1, strides=[16, 1])\n",
    "\n",
    "    def intrin_func(ins, outs):\n",
    "        ib = tvm.tir.ir_builder.create()\n",
    "        BA = ins[0]\n",
    "        BC = outs[0]\n",
    "        ib.emit(\n",
    "            tvm.tir.call_intrin(\n",
    "                \"handle\", \"tir.tvm_load_matrix_sync\",\n",
    "                BC.data, n, n, n,\n",
    "                BC.elem_offset // 256,\n",
    "                BA.access_ptr(\"r\"),\n",
    "                64,\n",
    "                \"col_major\"\n",
    "            )\n",
    "        )\n",
    "        return ib.get()\n",
    "    return te.decl_tensor_intrin(C.op, intrin_func, binds={A: BA, C: BC})\n",
    "\n",
    "s[AF].tensorize(hafi, intrin_wmma_load_matrix_a(\"wmma.matrix_a\"))\n",
    "s[WF].tensorize(wwf, intrin_wmma_load_matrix_b(\"wmma.matrix_b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primfn(A_1: handle, W_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {B: Buffer(B_2: Pointer(float32), float32, [8192, 512], []),\n",
      "             A: Buffer(A_2: Pointer(float16), float16, [8192, 256], []),\n",
      "             W: Buffer(W_2: Pointer(float16), float16, [512, 256], [])}\n",
      "  buffer_map = {A_1: A, W_1: W, B_1: B} {\n",
      "  attr [IterVar(blockIdx.x: int32, (nullptr), \"ThreadIndex\", \"blockIdx.x\")] \"thread_extent\" = 32;\n",
      "  allocate(B.wmma.accumulator: Pointer(wmma.accumulator float32), float32, [4096]), storage_scope = wmma.accumulator;\n",
      "  allocate(A.shared: Pointer(shared float16), float16, [16384]), storage_scope = shared;\n",
      "  allocate(W.shared: Pointer(shared float16), float16, [8192]), storage_scope = shared;\n",
      "  allocate(A.shared.wmma.matrix_a: Pointer(wmma.matrix_a float16), float16, [1024]), storage_scope = wmma.matrix_a;\n",
      "  allocate(W.shared.wmma.matrix_b: Pointer(wmma.matrix_b float16), float16, [1024]), storage_scope = wmma.matrix_b;\n",
      "  attr [IterVar(blockIdx.y: int32, (nullptr), \"ThreadIndex\", \"blockIdx.y\")] \"thread_extent\" = 4;\n",
      "  attr [IterVar(threadIdx.y: int32, (nullptr), \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 8 {\n",
      "    for (h.c.outer.init: int32, 0, 4) {\n",
      "      for (w.c.outer.init: int32, 0, 4) {\n",
      "        @tir.tvm_fill_fragment(B.wmma.accumulator, 16, 16, 16, ((h.c.outer.init*4) + w.c.outer.init), 0f32, dtype=handle)\n",
      "      }\n",
      "    }\n",
      "    for (hdr.outer.outer: int32, 0, 4) {\n",
      "      for (ax0.ax1.fused.outer.outer: int32, 0, 8) {\n",
      "        attr [IterVar(threadIdx.x: int32, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 32;\n",
      "        A.shared[ramp((((ax0.ax1.fused.outer.outer*2048) + (threadIdx.y*256)) + (threadIdx.x*8)), 1, 8)] = (float16x8*)A_2[ramp(((((((blockIdx.x*65536) + (ax0.ax1.fused.outer.outer*8192)) + (threadIdx.y*1024)) + (floordiv(threadIdx.x, 8)*256)) + (hdr.outer.outer*64)) + (floormod(threadIdx.x, 8)*8)), 1, 8)]\n",
      "      }\n",
      "      for (ax0.ax1.fused.outer.outer_1: int32, 0, 4) {\n",
      "        attr [IterVar(threadIdx.x, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 32;\n",
      "        W.shared[ramp((((ax0.ax1.fused.outer.outer_1*2048) + (threadIdx.y*256)) + (threadIdx.x*8)), 1, 8)] = (float16x8*)W_2[ramp(((((((blockIdx.y*32768) + (ax0.ax1.fused.outer.outer_1*8192)) + (threadIdx.y*1024)) + (floordiv(threadIdx.x, 8)*256)) + (hdr.outer.outer*64)) + (floormod(threadIdx.x, 8)*8)), 1, 8)]\n",
      "      }\n",
      "      for (hdr.outer.inner: int32, 0, 4) {\n",
      "        for (ax0.outer: int32, 0, 4) {\n",
      "          @tir.tvm_load_matrix_sync(A.shared.wmma.matrix_a, 16, 16, 16, ax0.outer, @tir.tvm_access_ptr(@tir.type_annotation(, dtype=float16), A.shared, (((floordiv(threadIdx.y, 2)*4096) + (ax0.outer*1024)) + (hdr.outer.inner*16)), 1024, 1, dtype=handle), 64, \"row_major\", dtype=handle)\n",
      "        }\n",
      "        for (ax0.outer_1: int32, 0, 4) {\n",
      "          @tir.tvm_load_matrix_sync(W.shared.wmma.matrix_b, 16, 16, 16, ax0.outer_1, @tir.tvm_access_ptr(@tir.type_annotation(, dtype=float16), W.shared, (((floormod(threadIdx.y, 2)*4096) + (ax0.outer_1*1024)) + (hdr.outer.inner*16)), 1024, 1, dtype=handle), 64, \"col_major\", dtype=handle)\n",
      "        }\n",
      "        for (h.c.outer: int32, 0, 4) {\n",
      "          for (w.c.outer: int32, 0, 4) {\n",
      "            @tir.tvm_mma_sync(B.wmma.accumulator, ((h.c.outer*4) + w.c.outer), A.shared.wmma.matrix_a, h.c.outer, W.shared.wmma.matrix_b, w.c.outer, B.wmma.accumulator, ((h.c.outer*4) + w.c.outer), dtype=handle)\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (h.inner.inner.outer: int32, 0, 4) {\n",
      "      for (w.inner.inner.outer: int32, 0, 4) {\n",
      "        @tir.tvm_store_matrix_sync(B.wmma.accumulator, 16, 16, 16, ((h.inner.inner.outer*4) + w.inner.inner.outer), @tir.tvm_access_ptr(@tir.type_annotation(, dtype=float32), B_2, ((((((blockIdx.x*131072) + (floordiv(threadIdx.y, 2)*32768)) + (h.inner.inner.outer*8192)) + (blockIdx.y*128)) + (floormod(threadIdx.y, 2)*64)) + (w.inner.inner.outer*16)), 8192, 2, dtype=handle), 512, \"row_major\", dtype=handle)\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.lower(s, [A, W, B], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time summary:\n",
      " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
      "   0.1358       0.1332       0.1447       0.1331       0.0045   \n",
      "               \n",
      "Matmul: 0.133138 ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tvm.testing\n",
    "dev = tvm.cuda(0)\n",
    "func = tvm.build(s, [A, W, B], \"cuda\")\n",
    "\n",
    "a_np = np.random.uniform(size=(n, hd)).astype(A.dtype)\n",
    "w_np = np.random.uniform(size=(wd, hd)).astype(W.dtype)\n",
    "\n",
    "a = tvm.nd.array(a_np, dev)\n",
    "w = tvm.nd.array(w_np, dev)\n",
    "\n",
    "b = tvm.nd.array(np.zeros((n, wd), dtype=B.dtype), dev)\n",
    "\n",
    "func(a, w, b)\n",
    "\n",
    "evaluator = func.time_evaluator(func.entry_name, dev, repeat=20, number=300)\n",
    "print(evaluator(a, w, b))\n",
    "print(\"Matmul: %f ms\" % (evaluator(a, w, b).mean * 1e3))\n",
    "\n",
    "tvm.testing.assert_allclose(b.numpy(), np.matmul(a.numpy(), np.transpose(w.numpy())), rtol = 0.001)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
